600
train loss:1.064965485734118
train loss:1.0180527585380827
train loss:1.044698208509741
train loss:1.0995600906487255
train loss:1.0548397645011718
train loss:1.0725728602121587
train loss:1.0450755975462607
train loss:1.0414357945922839
train loss:1.0313168550278347
train loss:1.041762038030269
train loss:1.0184852231805561
train loss:1.0342191842909267
train loss:0.9772152021703912
train loss:0.9494054104871023
train loss:0.9801008627511686
train loss:0.9422080771921337
train loss:1.009400065010854
train loss:0.9237035069396492
train loss:0.9133623702658924
train loss:0.9042350148140518
train loss:0.8648269770853951
train loss:0.8577318155570552
train loss:0.8393043884133307
train loss:0.7838819794195722
train loss:0.7913783327974668
train loss:0.7689451931881598
train loss:0.6918892258390568
train loss:0.6538785984968328
train loss:0.7104396317493881
train loss:0.6870961342967354
train loss:0.6231340410337003
train loss:0.6391816939779278
train loss:0.5983737561841478
train loss:0.6358489477154822
train loss:0.60562746205193
train loss:0.5733208003146497
train loss:0.44388677182105984
train loss:0.5456616836253475
train loss:0.5419657281442
train loss:0.402862302342838
train loss:0.46138433430919745
train loss:0.4026580954956463
train loss:0.3406725629590604
train loss:0.43943522167603943
train loss:0.3497743977798736
train loss:0.30450177224384506
train loss:0.5016252735812631
train loss:0.3969682453558578
train loss:0.3604558761215956
train loss:0.42711342610677305
train loss:0.37854473915322506
train loss:0.406654426145868
train loss:0.4090129614458886
train loss:0.32781224195925035
train loss:0.3109382978580787
train loss:0.34367172218594305
train loss:0.3408498645702459
train loss:0.35768688339736515
train loss:0.2834593442706462
train loss:0.23106294491364818
60
60
=== epoch:1, train acc:0.906, test acc:0.891 ===
train loss:0.23505743226638473
train loss:0.2223660094556631
train loss:0.2800169095833381
train loss:0.2894213769116559
train loss:0.34647234131462895
train loss:0.2277540789977062
train loss:0.2833966639814085
train loss:0.30385744790098385
train loss:0.28056740579300254
train loss:0.2169606354612127
train loss:0.28287382134184536
train loss:0.28187577606434266
train loss:0.2264837399200341
train loss:0.291533687293742
train loss:0.17238574226987208
train loss:0.2514097151274435
train loss:0.1869791877089323
train loss:0.24167573132703285
train loss:0.1698272545377585
train loss:0.18453454079217552
train loss:0.20671333608019526
train loss:0.1781081012214347
train loss:0.2227006629213675
train loss:0.23361776433518588
train loss:0.15358723556265988
train loss:0.24512674150863742
train loss:0.1699627810245831
train loss:0.19983377019813606
train loss:0.17777036337622915
train loss:0.19916237933975509
train loss:0.06562784361405599
train loss:0.15991829402081972
train loss:0.2590351949269532
train loss:0.1809074329415424
train loss:0.2198055467844365
train loss:0.13756413512705504
train loss:0.21735272882770137
train loss:0.22213420993929117
train loss:0.2059693365761686
train loss:0.12085838020557507
train loss:0.09438825602855495
train loss:0.14295267925908142
train loss:0.16264129269560512
train loss:0.18408953924745927
train loss:0.18118196207200338
train loss:0.11609017180162992
train loss:0.11121464562519513
train loss:0.13859767990819766
train loss:0.08583244731922694
train loss:0.13285524193741222
train loss:0.11390670435487107
train loss:0.11487340330886499
train loss:0.09565123451963332
train loss:0.1634684955184224
train loss:0.05254870799905828
train loss:0.12210326231497459
train loss:0.09510197481359532
train loss:0.11663130212690928
train loss:0.1486112460959906
train loss:0.11483124129663837
120
60
=== epoch:2, train acc:0.953, test acc:0.942 ===
train loss:0.06351604781020283
train loss:0.1799460447735567
train loss:0.10764070941615944
train loss:0.09629943373840605
train loss:0.1414872870035442
train loss:0.2589770144385255
train loss:0.060310066247565255
train loss:0.07165824371061125
train loss:0.11911476361187269
train loss:0.07661199550606276
train loss:0.060806804690934003
train loss:0.12992275672507564
train loss:0.052664412810712574
train loss:0.06414639610047942
train loss:0.06804994574534744
train loss:0.11286779968783815
train loss:0.12629674742168667
train loss:0.16951582516208805
train loss:0.06797624847017973
train loss:0.065922691366801
train loss:0.16170819596111471
train loss:0.13400817454806893
train loss:0.11558385389623617
train loss:0.13430907666607186
train loss:0.08260857369693715
train loss:0.08187788699038069
train loss:0.2764228133879591
train loss:0.13674317349889797
train loss:0.12558747663495945
train loss:0.045080677033097115
train loss:0.12317236477390629
train loss:0.1315289113534435
train loss:0.0368750714348574
train loss:0.09818073794244123
train loss:0.1496893378860578
train loss:0.08095077316719498
train loss:0.09915620160956996
train loss:0.11690746289330885
train loss:0.10390659025408241
train loss:0.0750528149950362
train loss:0.04359653928480941
train loss:0.05873884385933649
train loss:0.05713354581998619
train loss:0.0466047578231389
train loss:0.046927635283657294
train loss:0.08494112188892804
train loss:0.07975736518686696
train loss:0.059236789276485464
train loss:0.05622141502230726
train loss:0.14828240037115978
train loss:0.08552745146149951
train loss:0.09384079092992943
train loss:0.06278086079275087
train loss:0.12604069928245948
train loss:0.12384073960272576
train loss:0.04611741067983635
train loss:0.14167029317970453
train loss:0.05430387752899819
train loss:0.06381904984903465
train loss:0.03856588286713779
180
60
=== epoch:3, train acc:0.976, test acc:0.958 ===
train loss:0.04478823357809744
train loss:0.03305186587462888
train loss:0.07133092407394043
train loss:0.05020269452987789
train loss:0.14442225331688646
train loss:0.05457215718953779
train loss:0.07547516267420364
train loss:0.04918150621457848
train loss:0.07494951095157379
train loss:0.026848101545183203
train loss:0.07822040051033313
train loss:0.06479257204077155
train loss:0.03271123947904743
train loss:0.044290614898170905
train loss:0.04671285425930903
train loss:0.07890463560082708
train loss:0.041347709654507604
train loss:0.03302933565844443
train loss:0.027049886319291443
train loss:0.02560123583416171
train loss:0.057840904398776226
train loss:0.08355279861268321
train loss:0.030569580767138067
train loss:0.0781601934066075
train loss:0.08786086515239128
train loss:0.04291229770364878
train loss:0.05015907381296997
train loss:0.09043261977013833
train loss:0.018189730074648077
train loss:0.0726027873975217
train loss:0.03810977649056708
train loss:0.06610908471109485
train loss:0.04409186377527059
train loss:0.03242887994116462
train loss:0.033019304247748536
train loss:0.02336147231636535
train loss:0.03517024229984242
train loss:0.020537942263562946
train loss:0.04281862047476362
train loss:0.10797882863818678
train loss:0.06888894954074849
train loss:0.019582430509782457
train loss:0.05938069910135061
train loss:0.062150432394912254
train loss:0.1021674854109229
train loss:0.03634666733903882
train loss:0.053706525279306305
train loss:0.030806382026142796
train loss:0.01648862750921014
train loss:0.040038039478609184
train loss:0.08530792791242686
train loss:0.029346735414844834
train loss:0.018288917045531147
train loss:0.05594397571294563
train loss:0.0341285815496106
train loss:0.06197858384033381
train loss:0.012731163225385318
train loss:0.16896963642744242
train loss:0.06821943196489062
train loss:0.04843021461237786
240
60
=== epoch:4, train acc:0.987, test acc:0.971 ===
train loss:0.042262150316436264
train loss:0.06427307149419467
train loss:0.04615144477217183
train loss:0.04788973919367427
train loss:0.05706682741054252
train loss:0.03481085193481367
train loss:0.06433256727211557
train loss:0.030450198834266817
train loss:0.023388422775929212
train loss:0.0773541883098376
train loss:0.09061137857941468
train loss:0.03413791225282531
train loss:0.02343193027593636
train loss:0.019580515108474413
train loss:0.025815406197227303
train loss:0.039785620168536436
train loss:0.04496562821811276
train loss:0.018665962408610362
train loss:0.02782596969615777
train loss:0.02201647785500495
train loss:0.050209937341771226
train loss:0.01682599489487741
train loss:0.03161041046442934
train loss:0.019857625613019663
train loss:0.017007818582707848
train loss:0.042114618390005215
train loss:0.02037418987797195
train loss:0.013213679557501787
train loss:0.015871913007803044
train loss:0.04619676790926055
train loss:0.015773370883656494
train loss:0.04254089282660074
train loss:0.017331428585133538
train loss:0.01690554557285097
train loss:0.022365872841521396
train loss:0.015959536289257687
train loss:0.02277777252839233
train loss:0.010948341372928567
train loss:0.025400034008677738
train loss:0.019352097712048558
train loss:0.021056222874672518
train loss:0.017914958589202085
train loss:0.021985338734828674
train loss:0.023802231375565697
train loss:0.0236360337851969
train loss:0.028016106047089342
train loss:0.0071710561382409355
train loss:0.010104140214452929
train loss:0.011883278746828492
train loss:0.06718948094090425
train loss:0.010401239432840648
train loss:0.006450266307946289
train loss:0.021191753269409523
train loss:0.016611427050696344
train loss:0.011284504929722378
train loss:0.008481164313992607
train loss:0.042888690481264355
train loss:0.014313151710002388
train loss:0.03623681861926697
train loss:0.01717161275050227
300
60
=== epoch:5, train acc:0.993, test acc:0.975 ===
train loss:0.01704286517505668
train loss:0.009064082324805272
train loss:0.005850651582872567
train loss:0.054819665722919214
train loss:0.13041450868766077
train loss:0.024824481509256056
train loss:0.02049862527848094
train loss:0.01548687625753783
train loss:0.00680686670570648
train loss:0.015832062578419282
train loss:0.020628055630216368
train loss:0.012647595423395693
train loss:0.016289824970947647
train loss:0.014839565064375709
train loss:0.049075328247854474
train loss:0.02651181684554828
train loss:0.04871987624235799
train loss:0.011079300376890044
train loss:0.013542572470417631
train loss:0.0326491697849097
train loss:0.024365252557669536
train loss:0.00878912881663698
train loss:0.011940928013603686
train loss:0.04221394895217998
train loss:0.00911214422697134
train loss:0.01612143870967075
train loss:0.040772597050469095
train loss:0.010999379000491123
train loss:0.01547928092695011
train loss:0.013334636663242893
train loss:0.010233199104872227
train loss:0.010684570605586108
train loss:0.02684633538985612
train loss:0.019662603155973037
train loss:0.027883388817281016
train loss:0.0070408084233621535
train loss:0.00961971100797374
train loss:0.02135012100659801
train loss:0.04479148455435501
train loss:0.010860681420302267
train loss:0.008847222024436245
train loss:0.010391259625239884
train loss:0.009177631352011077
train loss:0.00924128632922318
train loss:0.015506985930008526
train loss:0.020168266793225385
train loss:0.008217669356668608
train loss:0.007757143290536306
train loss:0.011679538326222587
train loss:0.011899668687608193
train loss:0.00664912615982755
train loss:0.008253360260831588
train loss:0.007170058729715863
train loss:0.07834602592862068
train loss:0.012593809636164703
train loss:0.013622016220697229
train loss:0.03585105754186643
train loss:0.03398638821566777
train loss:0.006372704383171651
train loss:0.045054389329772467
360
60
=== epoch:6, train acc:0.984, test acc:0.959 ===
train loss:0.00871824283301783
train loss:0.01599316482736548
train loss:0.014086076571480059
train loss:0.012088031722351148
train loss:0.010209560701293372
train loss:0.008513253920318532
train loss:0.010443603601703043
train loss:0.011740150836701595
train loss:0.004161545683449131
train loss:0.008828623120207298
train loss:0.011857945999583966
train loss:0.014881655555037492
train loss:0.006032258333152518
train loss:0.017500995416812354
train loss:0.018451836319855343
train loss:0.035356342328574486
train loss:0.014970314776176696
train loss:0.016638275821089153
train loss:0.015669382149064978
train loss:0.014029695200624142
train loss:0.009129243771031503
train loss:0.01815847691393741
train loss:0.004910459142535324
train loss:0.004897411900487844
train loss:0.013345371909562706
train loss:0.015056958835364216
train loss:0.002783617338678875
train loss:0.014314265902767154
train loss:0.007828367216264804
train loss:0.010038100834157902
train loss:0.004796609054681466
train loss:0.01070686655224381
train loss:0.0075149486988812355
train loss:0.010907815589351433
train loss:0.012014501822043977
train loss:0.005915659454393037
train loss:0.0068687610517726904
train loss:0.00771397730556404
train loss:0.008243649706000809
train loss:0.006266258727632258
train loss:0.0068179929741805844
train loss:0.008006305796456045
train loss:0.0051379071427203726
train loss:0.005162338647668919
train loss:0.02329259240057631
train loss:0.01337396626437912
train loss:0.004497362439892052
train loss:0.010079731868945159
train loss:0.008739758392485676
train loss:0.029052778842976232
train loss:0.008880027159213117
train loss:0.004552261131934735
train loss:0.00671995524573229
train loss:0.011200117632585486
train loss:0.0070920817327870785
train loss:0.004412698156701875
train loss:0.005891155724291865
train loss:0.004532964573491649
train loss:0.004349588211747937
train loss:0.0023377251283289704
420
60
=== epoch:7, train acc:0.999, test acc:0.976 ===
train loss:0.006312631197373863
train loss:0.004791114440434397
train loss:0.0029201901815412933
train loss:0.004879696307227884
train loss:0.01267018147088189
train loss:0.004280005685784203
train loss:0.0070237498594255
train loss:0.00272075123706369
train loss:0.0036709899494611793
train loss:0.003034539933619223
train loss:0.003906047617318525
train loss:0.02408602587171015
train loss:0.01791977014329673
train loss:0.005074822351858066
train loss:0.00869278206814774
train loss:0.006042668498155614
train loss:0.004603436124297225
train loss:0.004078997759154499
train loss:0.007097032011354899
train loss:0.005502701968778374
train loss:0.007285806268924102
train loss:0.0047057158905732385
train loss:0.007025702594002461
train loss:0.006665392226893147
train loss:0.005955745588250302
train loss:0.0036168307068608074
train loss:0.00446216614494147
train loss:0.0036809977806730827
train loss:0.003630172415617962
train loss:0.0016179796097895614
train loss:0.006442057681634626
train loss:0.007447434025795318
train loss:0.00463298576912517
train loss:0.0028193296377765037
train loss:0.00430942404789249
train loss:0.003918999965348632
train loss:0.003587429425431305
train loss:0.00311586853864656
train loss:0.005854819481592058
train loss:0.002840852486074897
train loss:0.0036029956547890267
train loss:0.03776015030434407
train loss:0.004129970734792699
train loss:0.002311942468582205
train loss:0.004275576846742176
train loss:0.00532531434687359
train loss:0.0014399939116374455
train loss:0.002263964269489544
train loss:0.0066980900411628295
train loss:0.003706355515262464
train loss:0.005010799188260101
train loss:0.002562288651973679
train loss:0.004237365995704701
train loss:0.007439708732125307
train loss:0.002765150505481162
train loss:0.00504385171124009
train loss:0.02541597860845987
train loss:0.0031365573185586754
train loss:0.013935353359576845
train loss:0.0019287053280300532
480
60
=== epoch:8, train acc:0.998, test acc:0.976 ===
train loss:0.006689010416972308
train loss:0.010284381739595748
train loss:0.003398319731469422
train loss:0.0024540268383797317
train loss:0.004485699529294816
train loss:0.003729836933450246
train loss:0.002668583534991264
train loss:0.004279966977392656
train loss:0.002519571794770748
train loss:0.0016067607241937423
train loss:0.002874188577247892
train loss:0.004884966292990605
train loss:0.0010800771109057074
train loss:0.004653494775892792
train loss:0.0033977376223133433
train loss:0.0020666685014558046
train loss:0.0026584546444299046
train loss:0.007570083066375419
train loss:0.0031366809297051733
train loss:0.032646250049506616
train loss:0.0012730917809476168
train loss:0.0015950349528501267
train loss:0.013256959325698774
train loss:0.001472394867715232
train loss:0.004706217578108778
train loss:0.006184952397765236
train loss:0.0032221581880523408
train loss:0.0022463997932546274
train loss:0.003124837211183772
train loss:0.0054903137107209345
train loss:0.0036833402110921384
train loss:0.017902125438589794
train loss:0.00329537722363177
train loss:0.0018076591738286928
train loss:0.0038724383059354005
train loss:0.004177558826726856
train loss:0.0033382083685311328
train loss:0.0027355808283014136
train loss:0.0032495345789928555
train loss:0.007280782389684707
train loss:0.004285141021704404
train loss:0.0018274851373675984
train loss:0.010620959393372572
train loss:0.006915812563293731
train loss:0.0024997838154830238
train loss:0.0033706789982906903
train loss:0.002022289718531748
train loss:0.002049407286502979
train loss:0.003795271555104249
train loss:0.001383526691045376
train loss:0.0019344455441213564
train loss:0.002195100376541251
train loss:0.0032801267856041494
train loss:0.002032102565764161
train loss:0.0013221729585955195
train loss:0.002160252786453709
train loss:0.00187456953374782
train loss:0.0008493387294692113
train loss:0.002611503884995774
train loss:0.0011845811446814655
540
60
=== epoch:9, train acc:1.0, test acc:0.978 ===
train loss:0.003439877287678331
train loss:0.0012816188156909488
train loss:0.0012937955767953254
train loss:0.005336169030721599
train loss:0.0035427835618369475
train loss:0.004893711497940811
train loss:0.0011249432142442014
train loss:0.002725075185823319
train loss:0.0032983659427874828
train loss:0.0019474129153355343
train loss:0.0022141273311173867
train loss:0.0037301362564995744
train loss:0.0021172650975736025
train loss:0.0028568064690102923
train loss:0.002062329070489934
train loss:0.002365650142856115
train loss:0.0026671310422992068
train loss:0.0007149988999131051
train loss:0.0018327502046987306
train loss:0.001359818402493724
train loss:0.00247514771345848
train loss:0.0013641960795360464
train loss:0.0019210221046173045
train loss:0.0031554053303624492
train loss:0.00276734205999691
train loss:0.0046530565899750255
train loss:0.0020978088373031652
train loss:0.0013132427197472324
train loss:0.0014182026622483219
train loss:0.0018158476670129956
train loss:0.002011910427278091
train loss:0.001958294861690338
train loss:0.0012326258407056063
train loss:0.004470126818284524
train loss:0.000774599622094998
train loss:0.002666933204342825
train loss:0.0014525198764507139
train loss:0.0013501477380826572
train loss:0.0020671574072737386
train loss:0.001876219357562919
train loss:0.0016854706520050544
train loss:0.0009605622180295629
train loss:0.004075257150116278
train loss:0.0013748210853795982
train loss:0.0012826289786241442
train loss:0.0008378210656616319
train loss:0.0007405217165196
train loss:0.0011457117345614553
train loss:0.003662915108537378
train loss:0.0024888189259068896
train loss:0.0016125754517778625
train loss:0.001565267020714173
train loss:0.0018004088204255674
train loss:0.0006527481543710985
train loss:0.0032017819626626604
train loss:0.001706068247371445
train loss:0.00221996476275209
train loss:0.001255724558211074
train loss:0.08933405205582216
train loss:0.0015631545579512678
600
60
=== epoch:10, train acc:1.0, test acc:0.976 ===
=============== Final Test Accuracy ===============
test acc:0.9708609271523179
Saved Network Parameters!
