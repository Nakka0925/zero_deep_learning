train_accuracy: 0.4076158940397351 test_accuracy: 0.38609271523178806
epoch1 33380.30155849457s
(7550, 1, 192, 192)
train loss:1.0631734731050393
=== epoch:1, train acc:0.406, test acc:0.401 ===
train loss:1.085642573983513
train loss:1.0575148144980293
train loss:1.0293469846459207
train loss:1.090644042034813
train loss:1.069363511723048
train loss:0.9984078043852963
train loss:1.0492779466825415
train loss:1.0697020801209813
train loss:1.0521459365899408
train loss:1.0117499350234176
train loss:1.060792010877211
train loss:1.0576333409988898
train loss:1.063514964623937
train loss:1.0413361534162264
train loss:1.0285577298200668
train loss:1.0155384783337313
train loss:1.05529736964703
train loss:1.047099105032852
train loss:1.0648005258889897
train loss:1.0351604245571193
train loss:1.0220251097420319
train loss:1.0193256420655998
train loss:1.0354247710439772
train loss:1.0715604593981671
train loss:1.0343542518897026
train loss:1.0441420372251988
train loss:1.0417649534059807
train loss:1.0001560731441956
train loss:0.9947902897403254
train loss:1.005852439493614
train loss:0.9966828488374391
train loss:1.003045847633122
train loss:1.0375467334517563
train loss:1.0004649110134465
train loss:0.976008303546523
train loss:1.0250115701404572
train loss:1.0241105986748074
train loss:0.9851911188722707
train loss:1.0187364456742314
train loss:0.9772666946915706
train loss:0.970220988600583
train loss:0.9356778770943497
train loss:0.9642562166512059
train loss:0.9348343078240684
train loss:0.9301214139215869
train loss:0.9209783660927487
train loss:0.8748996329122747
train loss:0.8390422401558573
train loss:0.8396555751630741
train loss:0.890879473137191
train loss:0.8709703653495642
train loss:0.8257116962120974
train loss:0.8120544025640519
train loss:0.7847291859025973
train loss:0.7698381799875346
train loss:0.7820435989102039
train loss:0.6848934699398543
train loss:0.7734300198250383
train loss:0.6780332940622859
train loss:0.7542249675648818
train loss:0.6767267357833324
train loss:0.6751338686535671
train loss:0.5729449455768009
train loss:0.526335498971314
train loss:0.597312932314871
train loss:0.586409282494765
train loss:0.5291305110257265
train loss:0.4714561585958809
train loss:0.5481668085288907
train loss:0.5939022484730708
train loss:0.44840867187904215
train loss:0.4232720369431995
train loss:0.41207873315970467
train loss:0.38361535215320025
train loss:0.39622388616161763
train loss:0.37963985181596355
train loss:0.4048675000498193
train loss:0.30907753135251714
train loss:0.3161314400150136
train loss:0.3649516624672382
train loss:0.2977901933030337
train loss:0.44386944358291297
train loss:0.31190180763473047
train loss:0.3526032223077329
train loss:0.3807648553773624
train loss:0.4200187451509687
train loss:0.2996989275979477
train loss:0.3055933158493867
train loss:0.28652689243218893
train loss:0.30112421064797457
train loss:0.3532818285464427
train loss:0.2826059477843918
train loss:0.4197205770345635
train loss:0.3993719173893512
train loss:0.3353359103604761
train loss:0.33712917270114473
train loss:0.27737414568461505
train loss:0.3080185672528811
train loss:0.29122217519046834
train loss:0.30911417660124574
train loss:0.2784628979447297
train loss:0.28369283176153187
train loss:0.32342912811466495
train loss:0.4005827070654499
train loss:0.3394357645305475
train loss:0.22799265452266276
train loss:0.28416955836730945
train loss:0.2876729663156528
train loss:0.26270536710434983
train loss:0.2349111564665439
train loss:0.198405997651368
train loss:0.1574528750822988
train loss:0.16205659244687382
train loss:0.2541974455271752
train loss:0.21381349453509405
train loss:0.16998542446722686
train loss:0.18576381039798712
train loss:0.16269050292884069
train loss:0.21155659389633424
train loss:0.22786786564556963
train loss:0.20213762523852918
train loss:0.18497727731526237
train loss:0.18380779513800508
train loss:0.4172080096055176
train loss:0.16369392201107144
train loss:0.2081637347672516
train loss:0.21741141317565177
train loss:0.2628520984829489
train loss:0.2328569205702604
train loss:0.24978095567988098
train loss:0.294529633532929
train loss:0.2292466130868909
train loss:0.1624022313136525
train loss:0.16794399476177074
train loss:0.16619042360116296
train loss:0.17654003077327832
train loss:0.10855008978893643
train loss:0.18508997697647872
train loss:0.1658158159473987
train loss:0.22762255914708668
train loss:0.2893707479449903
train loss:0.19499049084831757
train loss:0.16891416981878593
train loss:0.2405287831009884
train loss:0.27406177501074386
train loss:0.11824059623432938
train loss:0.14727609151206722
train loss:0.26486698193225666
train loss:0.14275407730063455
train loss:0.1734989159279208
train loss:0.1610341954801183
train loss:0.15848224587308166
train loss:0.14924165588301044
train loss:0.20169491591025934
train loss:0.16806030739602906
train loss:0.15545463955037822
train loss:0.10682622784033464
train loss:0.24475896155635987
train loss:0.19512699368016817
train loss:0.08805095502581756
train loss:0.12152489550489194
train loss:0.1982336877971565
train loss:0.12301451671612272
train loss:0.13428117691067148
train loss:0.17076844415978126
train loss:0.1372787479201103
train loss:0.1962190565118286
train loss:0.1317164803759102
train loss:0.11766822918172525
train loss:0.1033031542172976
train loss:0.11380663661018961
train loss:0.10878863752918122
train loss:0.12932527108686095
train loss:0.12089425972214872
train loss:0.21017728750142914
train loss:0.12410788772344825
train loss:0.09478026185239412
train loss:0.12193100101652493
train loss:0.16711245681055575
train loss:0.12183109799910712
train loss:0.08720382787599981
train loss:0.14008663505316019
train loss:0.1713315771694865
train loss:0.11472779772754428
train loss:0.1460295742658934
train loss:0.14687112395482135
train loss:0.1963567835427318
train loss:0.15458675227603252
train loss:0.1704876676202447
train loss:0.12382935357049654
train loss:0.18876200228706463
train loss:0.10545717746608195
train loss:0.1154182138409645
train loss:0.10235595061260103
train loss:0.16876506678701733
train loss:0.06324708322616118
train loss:0.07025748094553495
train loss:0.08782456749322072
train loss:0.13418289690392682
train loss:0.10353681913695462
train loss:0.17584160368163185
train loss:0.10846688183982131
train loss:0.10270240131876524
train loss:0.04956314262815789
train loss:0.22324847881626095
train loss:0.050912166381977556
train loss:0.18032098605233884
train loss:0.1281102483047016
train loss:0.08386846884782402
train loss:0.07239639331258783
train loss:0.11540969766628802
train loss:0.09847046932131709
train loss:0.093210149153711
train loss:0.09010228535594632
train loss:0.09026790022730806
train loss:0.07809503195923571
train loss:0.1840471356113328
train loss:0.15004386964082675
train loss:0.09406423759386277
train loss:0.14247606783085554
train loss:0.10264044954367212
train loss:0.08166481558374997
train loss:0.08030829985863487
train loss:0.07723080117917443
train loss:0.16206060422060203
train loss:0.07516748704986123
train loss:0.04749969085710567
train loss:0.12300277373224994
train loss:0.056309962083403026
train loss:0.09478200253067587
train loss:0.09121276262338934
train loss:0.10877314856126023
train loss:0.1052492630359614
train loss:0.12011119972235386
train loss:0.13645356135133882
train loss:0.056851609001282416
train loss:0.09827298087774765
train loss:0.0675529115502792
train loss:0.07584417614032113
train loss:0.08726145421484245
train loss:0.13223616147930595
train loss:0.032323583133789004
train loss:0.1545150246236586
train loss:0.1690330472037499
train loss:0.06522686146953108
train loss:0.08146350091896183
train loss:0.10271217267879179
train loss:0.06547759959193415
train loss:0.12415868515428556
train loss:0.06599148459312709
train loss:0.0840806431467213
train loss:0.17178779905749952
train loss:0.08922059357944839
train loss:0.11944113049483449
train loss:0.09385112510896637
train loss:0.09365139174591229
train loss:0.10766863326511601
train loss:0.06130299882452949
train loss:0.03604086929910753
train loss:0.040394109558537804
train loss:0.08357754574057875
train loss:0.08040215403316958
train loss:0.08066101765468238
train loss:0.04824821282023261
train loss:0.07509086144608478
train loss:0.1142507526618341
train loss:0.06803601797915663
train loss:0.19308152458258032
train loss:0.08567272423733764
train loss:0.19792433703233314
train loss:0.09995242356360723
train loss:0.10651041225488594
train loss:0.09631324359909549
train loss:0.09409293758285255
train loss:0.1391328718767936
train loss:0.08117996622497886
train loss:0.09659787109351918
train loss:0.07695156236092134
train loss:0.03031866628553908
train loss:0.10227722677046418
train loss:0.03171064185149633
train loss:0.04802877516459247
train loss:0.07222262709535272
train loss:0.037305081933175915
train loss:0.06493176896102426
train loss:0.05589745581371121
train loss:0.05106012476334544
train loss:0.04280594407777752
train loss:0.022944071085044752
train loss:0.04999683605985053
train loss:0.12638606243636036
train loss:0.0756175978732699
train loss:0.093679701268086
train loss:0.05789898842958096
train loss:0.046830315329230536
train loss:0.057366191133437196
train loss:0.07169596490572784
train loss:0.16247927301791198
train loss:0.06273174631894285
train loss:0.06368795642168865
train loss:0.07935765938608826
=============== Final Test Accuracy ===============
test acc:0.9635761589403974
Saved Network Parameters!
Traceback (most recent call last):
  File "Cov_test.py", line 49, in <module>
    plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py", line 2769, in plot
    return gca().plot(
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py", line 1632, in plot
    lines = [*self._get_lines(*args, data=data, **kwargs)]
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py", line 312, in __call__
    yield from self._plot_args(this, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py", line 498, in _plot_args
    raise ValueError(f"x and y must have same first dimension, but "
ValueError: x and y must have same first dimension, but have shapes (5,) and (1,)
(7550, 1, 192, 192)
train loss:1.097295667959316
=== epoch:1, train acc:0.428, test acc:0.404 ===
train loss:1.0922010013433334
train loss:1.0747164229204575
train loss:1.0510565931564395
train loss:1.0939195178635253
train loss:1.0277253577605292
train loss:1.012947709741581
train loss:1.0104110120253742
train loss:1.059098775360764
train loss:1.070383254686608
train loss:1.0571792464334469
train loss:1.0803068262832114
train loss:1.0748451511974837
train loss:1.0607393009602344
train loss:1.0677518735647649
train loss:1.0561822902756968
train loss:1.0627198711080728
train loss:1.06062997678174
train loss:1.032056361259874
train loss:1.0820850620403621
train loss:1.1024875501508347
train loss:1.0421033802870394
train loss:1.0174386527781714
train loss:1.031239830948382
train loss:1.050648217808013
train loss:1.0661781150665597
train loss:1.053752220744278
train loss:1.0763290099196363
train loss:1.0470947321131383
train loss:1.0866616539066616
train loss:1.0468748750848398
train loss:1.0426549881645777
train loss:1.056095070906581
train loss:0.9876305767226728
train loss:1.0864354579551863
train loss:1.0298564117320357
train loss:1.0324972006334776
train loss:1.0092798234044755
train loss:1.022808448957177
train loss:1.0019308485268323
train loss:1.0155348341201569
train loss:1.0196107145167728
train loss:1.0002920310275416
train loss:0.9131862499035917
train loss:0.9991111709417068
train loss:0.936473585421853
train loss:0.946405065509825
train loss:1.0859179265854753
train loss:0.9524281531556507
train loss:0.9909293778824301
train loss:1.0007633466689017
train loss:0.9721851558731465
train loss:0.9809601231468994
train loss:0.9340833813955162
train loss:0.9155212050471009
train loss:0.9098974115237735
train loss:0.9160680202651068
train loss:0.865510474381499
train loss:0.9404211106545186
train loss:0.8464662850430296
train loss:0.7947885129863559
train loss:0.930163166662445
train loss:0.8703461043434778
train loss:0.7529897476468365
train loss:0.8161236054236397
train loss:0.7617692686359829
train loss:0.8379559349663706
train loss:0.7623545229530593
train loss:0.7961708886565302
train loss:0.7122165695640382
train loss:0.7778951106677039
train loss:0.6569060980016209
train loss:0.6775413072132114
train loss:0.695691846272892
train loss:0.6392448736712357
train loss:0.5796797090642649
train loss:0.6004813114394821
train loss:0.6185226856724504
train loss:0.548413557872279
train loss:0.6092620411156612
train loss:0.6048875525726768
train loss:0.5646732352486876
train loss:0.5293753317800575
train loss:0.4330994719786954
train loss:0.5603591370145531
train loss:0.4852158177155043
train loss:0.5261384414226549
train loss:0.44225356041171504
train loss:0.4493965122963339
train loss:0.48628141257821794
train loss:0.5240655754256834
train loss:0.5042341096516725
train loss:0.484935844772936
train loss:0.4106318093669229
train loss:0.42079348028311353
train loss:0.504892350306366
train loss:0.3565673281453948
train loss:0.47603961226973623
train loss:0.3788103407601637
train loss:0.4397058057378815
train loss:0.4197760045695132
train loss:0.3416838597029409
train loss:0.48729145520541833
train loss:0.43372929442727676
train loss:0.44828464095308207
train loss:0.4329986772634129
train loss:0.40854745195566394
train loss:0.28171713351110284
train loss:0.4371124741026705
train loss:0.36646515557286874
train loss:0.35476417186986614
train loss:0.295776864243166
train loss:0.5260713575944211
train loss:0.21562790430381046
train loss:0.28130203130002857
train loss:0.36152187736779484
train loss:0.30120863423341526
train loss:0.42765788216372314
train loss:0.2555953526624454
train loss:0.3875336653992022
train loss:0.23342545552432373
train loss:0.2854608162940823
train loss:0.19850343156032507
train loss:0.25127988245057264
train loss:0.32009424792299734
train loss:0.28253981810218376
train loss:0.2136592591632841
train loss:0.21825871073002945
train loss:0.27397519236670803
train loss:0.30967604200874577
train loss:0.2639857702728354
train loss:0.16447383552744213
train loss:0.2605868640324955
train loss:0.3667644622986987
train loss:0.233142173120274
train loss:0.25702355506341457
train loss:0.33283929342311747
train loss:0.21115185256796612
train loss:0.29550152859869994
train loss:0.11516940647798153
train loss:0.2119387727891889
train loss:0.2524435221732257
train loss:0.15577986631693638
train loss:0.38037601091604223
train loss:0.26539079974406965
train loss:0.39173767188997916
train loss:0.2532019050145386
train loss:0.2093660739384186
train loss:0.20322576160755088
train loss:0.26405331281768407
train loss:0.22767032195650272
train loss:0.17208032649815688
train loss:0.28526885121025686
train loss:0.24961553600067923
train loss:0.21845013524618145
train loss:0.39979214722604917
train loss:0.18948424967384914
train loss:0.16510600330141037
train loss:0.14114611917768108
train loss:0.2531812795002988
train loss:0.3159567032748003
train loss:0.1679207963568801
train loss:0.16195032965953163
train loss:0.1871973311119102
train loss:0.15862871781887386
train loss:0.3031797824954516
train loss:0.22962452443018666
train loss:0.1879558041254994
train loss:0.18408546411673624
train loss:0.21850682449932332
train loss:0.22175434128053445
train loss:0.17220649616355702
train loss:0.1381532964529469
train loss:0.24737992381819163
train loss:0.21728970872845188
train loss:0.13024822793876734
train loss:0.14032764547003207
train loss:0.21408959603696318
train loss:0.1868562914962748
train loss:0.2341336385624713
train loss:0.12627988962076464
train loss:0.15465420536233965
train loss:0.20466146821174502
train loss:0.10693106788330399
train loss:0.10885933352528313
train loss:0.16407313467750598
train loss:0.12730342641531842
train loss:0.2810658234147427
train loss:0.2213079241226389
train loss:0.1626715479060695
train loss:0.14998199344990135
train loss:0.12955429486565548
train loss:0.10912682331493073
train loss:0.13095966606973364
train loss:0.1574737801719945
train loss:0.22945392165579415
train loss:0.12283471521649389
train loss:0.16015356567934583
train loss:0.12635998829679262
train loss:0.16801535796248931
train loss:0.1734871770372037
train loss:0.17771331953173367
train loss:0.12804985835827523
train loss:0.12828724993027443
train loss:0.19333158871719017
train loss:0.13578096791823252
train loss:0.1368516236246751
train loss:0.11455676826000151
train loss:0.11073033493493939
train loss:0.18167667532859474
train loss:0.1827714829876989
train loss:0.14828829261143667
train loss:0.17848467765687295
train loss:0.24098217558322482
train loss:0.13880985425868
train loss:0.1990477159512778
train loss:0.16521075075726654
train loss:0.14207514939602864
train loss:0.1296738652300115
train loss:0.16822695541588295
train loss:0.0703781461670488
train loss:0.06295687657793474
train loss:0.14620092636621362
train loss:0.12145740902536277
train loss:0.1376111815922409
train loss:0.10948694174576792
train loss:0.10689732821606368
train loss:0.08113146762131125
train loss:0.0669278787053262
train loss:0.10352841505206975
train loss:0.1258008094942677
train loss:0.1930045444284792
train loss:0.07628590558152953
train loss:0.16293119785667304
train loss:0.19223720356496696
train loss:0.2048748055646655
train loss:0.0750806831135374
train loss:0.10217894673171835
train loss:0.09887958947106557
train loss:0.13567712894365402
train loss:0.14032444155569632
train loss:0.0996643609308989
train loss:0.10276197569357029
train loss:0.08190569094569015
train loss:0.12168105657011148
train loss:0.1311070845089287
train loss:0.07826533820445425
train loss:0.10463804591675457
train loss:0.23593128970755664
train loss:0.08636498539896154
train loss:0.18892845605221545
train loss:0.08481253380673896
train loss:0.10093814236487612
train loss:0.06718556873301486
train loss:0.11383677683908613
train loss:0.09756877229999475
train loss:0.08754872525882314
train loss:0.11954280831025464
train loss:0.048805201073048726
train loss:0.11346584389701586
train loss:0.07207075642558508
train loss:0.09726509331155075
train loss:0.046392649937922324
train loss:0.0936110636578921
train loss:0.031947226280096726
train loss:0.10399887752412797
train loss:0.09303545023666086
train loss:0.12370760360033657
train loss:0.08532378083455384
train loss:0.05406391963997501
train loss:0.12335574248611443
train loss:0.07191715758785856
train loss:0.11051242782334249
train loss:0.10128268900329729
train loss:0.06931022532995207
train loss:0.1202678310670443
train loss:0.056775860871895185
train loss:0.12447794321567253
train loss:0.14270934794503087
train loss:0.104230673535119
train loss:0.1792738542884533
train loss:0.19083705635404552
train loss:0.048970372685888365
train loss:0.06299076899379519
train loss:0.07804183458032961
train loss:0.16890103641834983
train loss:0.053958391665293315
train loss:0.04664661950217125
train loss:0.1054605403270529
train loss:0.06654617600501385
train loss:0.08595185686905954
train loss:0.053185011570076046
train loss:0.15362786657906938
train loss:0.11045358620559893
train loss:0.07958231030687526
train loss:0.13289536601836788
train loss:0.08011265185316425
train loss:0.08630063253485414
train loss:0.10108230099480295
train loss:0.14285247358663644
train loss:0.08767482840010014
train loss:0.19465664855461498
=============== Final Test Accuracy ===============
test acc:0.9596026490066225
Saved Network Parameters!
Traceback (most recent call last):
  File "Cov_test.py", line 49, in <module>
    plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py", line 2769, in plot
    return gca().plot(
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py", line 1632, in plot
    lines = [*self._get_lines(*args, data=data, **kwargs)]
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py", line 312, in __call__
    yield from self._plot_args(this, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py", line 498, in _plot_args
    raise ValueError(f"x and y must have same first dimension, but "
ValueError: x and y must have same first dimension, but have shapes (5,) and (1,)
