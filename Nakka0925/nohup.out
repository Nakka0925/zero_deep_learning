train loss:1.0406825815910492
train loss:1.089340866861004
train loss:1.0582280472300045
train loss:1.1117345275682198
train loss:1.0809603101277714
train loss:1.0449773505233022
train loss:1.0573974688071888
train loss:1.0577977626258848
train loss:1.0459560614514372
train loss:1.009058216334448
train loss:1.0711933074789182
train loss:1.017562359209273
train loss:1.03376263849772
train loss:1.0732594113158787
train loss:1.0264436932389005
train loss:1.066822713984787
train loss:1.0162797610002483
train loss:0.9923543794059653
train loss:1.0484710752235507
train loss:1.0627665084995495
train loss:1.0185014598912967
train loss:1.0981507423663075
train loss:1.025973480326086
train loss:1.0053516059080871
train loss:1.0380799423697966
train loss:1.0414097308672066
train loss:1.0369301175994168
train loss:1.0088654397432069
train loss:1.078920122693184
train loss:1.0262123665497922
train loss:1.015560427884375
train loss:1.0207943889972821
train loss:1.0216124194405218
train loss:0.9952109226607491
train loss:0.9234729605757144
train loss:0.9486061266907746
train loss:0.9859707638448405
train loss:0.9353215464653145
train loss:1.0395186163333983
train loss:0.9924605826268541
train loss:1.0220614468698819
train loss:0.9670002784078268
train loss:0.976201257764957
train loss:0.9530183989047702
train loss:0.9682759564342797
train loss:0.9487450199788294
train loss:0.9219220101241322
train loss:0.9088866452562199
train loss:0.9279560284762748
train loss:0.9413987920776716
train loss:0.8433532543136592
train loss:0.8709959788689581
train loss:0.8407603878317889
train loss:0.7878367359346072
train loss:0.7923669855662294
train loss:0.7388759959123474
train loss:0.7355498224293744
train loss:0.7457721794417427
train loss:0.7798613940088124
train loss:0.6915560729477293
=== epoch:1, train acc:0.7405629139072848, test acc:0.7231788079470198 ===
train loss:0.7029710071455169
train loss:0.6755951588491002
train loss:0.724403405710245
train loss:0.5804950642629609
train loss:0.6700640716339531
train loss:0.6113016572592622
train loss:0.6796802040724829
train loss:0.5389911721599231
train loss:0.5439051724429698
train loss:0.4727666320279858
train loss:0.3853508010158905
train loss:0.4759949936225144
train loss:0.5346019591450577
train loss:0.5517170806363934
train loss:0.4461262265606203
train loss:0.49192803639784616
train loss:0.3216777667811005
train loss:0.5473081008922528
train loss:0.36607708490220603
train loss:0.5053814458489022
train loss:0.42839936619211
train loss:0.381433173422771
train loss:0.361835942435146
train loss:0.5295353355409693
train loss:0.3684960693031193
train loss:0.3037156688049063
train loss:0.3733398067088281
train loss:0.3969830182625428
train loss:0.4534157054600579
train loss:0.3110462086606087
train loss:0.302656575915101
train loss:0.3985822039172637
train loss:0.31722407087053306
train loss:0.3256340005269677
train loss:0.332795445382893
train loss:0.36774760277191526
train loss:0.28422497426677085
train loss:0.23364606858919962
train loss:0.3077250504264645
train loss:0.30132357929473125
train loss:0.40109936895524834
train loss:0.3137695991951798
train loss:0.3640593826625762
train loss:0.45500528280259794
train loss:0.31988241006574414
train loss:0.3828911666257131
train loss:0.24925842430652131
train loss:0.32571086370373353
train loss:0.36146629939125446
train loss:0.3797759488957674
train loss:0.23657371833653393
train loss:0.36788597286645974
train loss:0.28553556621529486
train loss:0.22278536016957942
train loss:0.27351830490611106
train loss:0.1611539076879075
train loss:0.16901426478499315
train loss:0.3265629035426398
train loss:0.3318083809127815
train loss:0.27881632284991664
=== epoch:2, train acc:0.8887417218543047, test acc:0.8927152317880794 ===
train loss:0.19081596786805372
train loss:0.18958396735425911
train loss:0.28609599962890475
train loss:0.25957055367854387
train loss:0.21469283407262518
train loss:0.15878726601667884
train loss:0.3008519434590382
train loss:0.1427824162953488
train loss:0.2137047440201522
train loss:0.22113490834533509
train loss:0.25358271627670353
train loss:0.2549644845274856
train loss:0.24049385864060568
train loss:0.28663345511414035
train loss:0.2788860682289136
train loss:0.16213979056611527
train loss:0.11053055932434158
train loss:0.33729333717661175
train loss:0.3339731658162788
train loss:0.22198838085539624
train loss:0.24674445329342143
train loss:0.20083430201095498
train loss:0.21913808980755484
train loss:0.2524228837006466
train loss:0.26497605636663796
train loss:0.16844299770262172
train loss:0.21035890972314278
train loss:0.21881065462245733
train loss:0.27692510490634986
train loss:0.17839703271704557
train loss:0.27200554934698806
train loss:0.17847169721766312
train loss:0.2086783910303311
train loss:0.12872812425693747
train loss:0.15883699510373273
train loss:0.3913361545674448
train loss:0.21756837204785714
train loss:0.1946376711429231
train loss:0.1941902092593748
train loss:0.158924966321115
train loss:0.15827119630854938
train loss:0.19210490232955388
train loss:0.2424121090152062
train loss:0.1927706210829752
train loss:0.1643847666193218
train loss:0.22434547151701864
train loss:0.2559411308853889
train loss:0.2636001747554565
train loss:0.19345119246705214
train loss:0.17865969074555546
train loss:0.13269546990870587
train loss:0.13982658496616188
train loss:0.1729497724838463
train loss:0.2039276891822653
train loss:0.18729187522045185
train loss:0.25838384741825154
train loss:0.17295507860835596
train loss:0.22031377041439523
train loss:0.21923192724791624
train loss:0.1914918113849727
=== epoch:3, train acc:0.9309602649006623, test acc:0.9105960264900662 ===
train loss:0.25661412957712554
train loss:0.16389774827602557
train loss:0.1947211730227237
train loss:0.1903237134744141
train loss:0.14700183763112815
train loss:0.18423689336209625
train loss:0.1727922349965433
train loss:0.16734478470591366
train loss:0.16632282585496777
train loss:0.20350349493781203
train loss:0.13891920590634432
train loss:0.1845956227887128
train loss:0.09954745142217797
train loss:0.18973276382442664
train loss:0.15992588114149295
train loss:0.16376261743325654
train loss:0.19685235244878452
train loss:0.16987793867866596
train loss:0.12613541650785218
train loss:0.13714287942951756
train loss:0.1770321405457441
train loss:0.09168614924183612
train loss:0.10542252743782082
train loss:0.2098473872686191
train loss:0.08638555391589932
train loss:0.09740780968518
train loss:0.22260753592283122
train loss:0.17731400199155928
train loss:0.1486193481523767
train loss:0.18465272140540606
train loss:0.17368639913978196
train loss:0.24567592655840173
train loss:0.15529624566816136
train loss:0.17280277019099116
train loss:0.13008834885518175
train loss:0.17865015594205075
train loss:0.13885574495105138
train loss:0.13423922768562924
train loss:0.14546152275254326
train loss:0.1628917213354122
train loss:0.13996093877278082
train loss:0.13682967290670495
train loss:0.12695946090836782
train loss:0.17804482678955946
train loss:0.1601168297407447
train loss:0.11966117500713194
train loss:0.14511188060275873
train loss:0.10465279042816497
train loss:0.18164461409450974
train loss:0.2081612389294051
train loss:0.10638749798956967
train loss:0.10952712532921344
train loss:0.16472124169500027
train loss:0.0822248706391508
train loss:0.10523297930274193
train loss:0.1402773810023503
train loss:0.2134181723062621
train loss:0.08460531827222795
train loss:0.1645107607426462
train loss:0.13455225552342198
=== epoch:4, train acc:0.9556291390728476, test acc:0.9430463576158941 ===
train loss:0.15161567830228728
train loss:0.20926890531620265
train loss:0.07460579894783774
train loss:0.155679349945742
train loss:0.1281647969467171
train loss:0.12007240395421792
train loss:0.13458266541866482
train loss:0.06340779133529015
train loss:0.06575186375122712
train loss:0.06837216938945972
train loss:0.11811512643522634
train loss:0.053777663802259526
train loss:0.06643845196549546
train loss:0.16283493407689112
train loss:0.08065514967293874
train loss:0.07372728640343264
train loss:0.06367371757366137
train loss:0.16013644147694342
train loss:0.08724343544284627
train loss:0.10653615610021241
train loss:0.10825366560725098
train loss:0.059972878374652906
train loss:0.0884594056786318
train loss:0.135660541075491
train loss:0.08461794754257843
train loss:0.10066099698382647
train loss:0.10752174164381706
train loss:0.06618410464157992
train loss:0.0947591206008118
train loss:0.13014407813046605
train loss:0.11290450505242186
train loss:0.05743662378943985
train loss:0.1002175611317659
train loss:0.057251633798514695
train loss:0.09584567005652199
train loss:0.060123644349541046
train loss:0.10374922264718638
train loss:0.10908882160695667
train loss:0.10510013545697827
train loss:0.05474808423470356
train loss:0.11655034194399383
train loss:0.12456715305091873
train loss:0.0646285499534417
train loss:0.03672612665537001
train loss:0.08832105692512134
train loss:0.060296551412113555
train loss:0.0897465826006992
train loss:0.07549147463868289
train loss:0.09053807947491517
train loss:0.0751991322323557
train loss:0.1273179791157162
train loss:0.12417426504114112
train loss:0.07584157863250594
train loss:0.04358402623669501
train loss:0.07902427478978184
train loss:0.12031930486771811
train loss:0.026365069851728547
train loss:0.08133539614656808
train loss:0.03470554403645378
train loss:0.060225751148018654
=== epoch:5, train acc:0.971523178807947, test acc:0.9582781456953643 ===
train loss:0.07814677703152902
train loss:0.10015447558532344
train loss:0.04761642101973014
train loss:0.11665411582830548
train loss:0.05103741485883123
train loss:0.13022072385141356
train loss:0.09185657226257589
train loss:0.07645288200515804
train loss:0.03920946001436856
train loss:0.0786699834556677
train loss:0.1065165999940309
train loss:0.15327271348608262
train loss:0.10728800760409542
train loss:0.08216551076279303
train loss:0.1281064456919894
train loss:0.10735723245695715
train loss:0.17195642446288292
train loss:0.07967607622511434
train loss:0.09062581588842201
train loss:0.03663344690084153
train loss:0.0867970674762863
train loss:0.06903205508053059
train loss:0.06153153302147898
train loss:0.06897419654856858
train loss:0.1341586235799921
train loss:0.1346307458955434
train loss:0.08950214072206694
train loss:0.051176481614231134
train loss:0.06933359963059506
train loss:0.040128514010481495
train loss:0.034299709376714255
train loss:0.06893842803198645
train loss:0.08884264074201637
train loss:0.1329179920700712
train loss:0.121291112471613
train loss:0.052403782033267315
train loss:0.0835954346815207
train loss:0.07850622261453806
train loss:0.04908509196142607
train loss:0.0706469830957331
train loss:0.1293267304322794
train loss:0.0731129039981286
train loss:0.03694238099290945
train loss:0.1496813461894731
train loss:0.05872896078351546
train loss:0.05039963285835979
train loss:0.08330755104006812
train loss:0.038229628136010274
train loss:0.11495058818351475
train loss:0.05695217088436829
train loss:0.03186329563208854
train loss:0.022538893325332918
train loss:0.03938420692090738
train loss:0.0432807137933584
train loss:0.029981455291454923
train loss:0.09912323070685196
train loss:0.03538946859883319
train loss:0.09204408156179955
train loss:0.06209707445098434
train loss:0.10369777620652093
=== epoch:6, train acc:0.9776490066225165, test acc:0.9649006622516556 ===
train loss:0.045473842193115524
train loss:0.05272153616592132
train loss:0.054990303739052504
train loss:0.05124056527206031
train loss:0.06132720934136475
train loss:0.12016045063016735
train loss:0.03494773395025651
train loss:0.06215504810642651
train loss:0.03502573159644132
train loss:0.06496629138732037
train loss:0.05588174274185829
train loss:0.06506192058581906
train loss:0.12942682644668
train loss:0.039291047365534966
train loss:0.1142650252153095
train loss:0.09108008210253678
train loss:0.059043230327002816
train loss:0.0459701422432635
train loss:0.021117910100251644
train loss:0.06120098110019633
train loss:0.1525480375618506
train loss:0.03632062456148067
train loss:0.02436245903385083
train loss:0.045371749988996644
train loss:0.03715918453162529
train loss:0.04711467549919443
train loss:0.029367480370432412
train loss:0.05226577308441125
train loss:0.026966302722464253
train loss:0.03231123588056056
train loss:0.1398722414632717
train loss:0.03309562239226532
train loss:0.0752614606529572
train loss:0.02534041893348511
train loss:0.03034161374970672
train loss:0.05006750920924643
train loss:0.052157620921653415
train loss:0.0572334325269347
train loss:0.07566670639564216
train loss:0.02041163184715529
train loss:0.02507929854499376
train loss:0.06842248695127605
train loss:0.061280570713441354
train loss:0.1563619274612447
train loss:0.030510826731099526
train loss:0.08308467682708474
train loss:0.03977548378902145
train loss:0.04152835843520378
train loss:0.016191230543865752
train loss:0.11702230734511711
train loss:0.020433880349062333
train loss:0.04076890123057985
train loss:0.05189757515938131
train loss:0.05050071497667079
train loss:0.0383240590075868
train loss:0.05393495395012244
train loss:0.024338069430915966
train loss:0.04640333993712386
train loss:0.07921314454858197
train loss:0.029759407319752306
=== epoch:7, train acc:0.9837748344370861, test acc:0.9682119205298013 ===
train loss:0.027606783095329802
train loss:0.05289889410372873
train loss:0.04773540126203505
train loss:0.030153997280420973
train loss:0.026304071309821854
train loss:0.02874241852230903
train loss:0.018934544175436493
train loss:0.02569617291756867
train loss:0.05554890635457294
train loss:0.024061983298508945
train loss:0.025976227443654142
train loss:0.03252537147593028
train loss:0.03173857290943447
train loss:0.04090711151596204
train loss:0.017495476133989562
train loss:0.019121233240689872
train loss:0.04026545101415606
train loss:0.13985816536997922
train loss:0.07083621178386496
train loss:0.033775311038629574
train loss:0.036582890668108645
train loss:0.03629219599242768
train loss:0.05438129508847344
train loss:0.055648786514495334
train loss:0.03920764011115879
train loss:0.026372603378318163
train loss:0.03258813594655682
train loss:0.08681745587357645
train loss:0.1008358308692428
train loss:0.13195451456717044
train loss:0.03970686007016736
train loss:0.018462096727844497
train loss:0.041189014497865904
train loss:0.025296930160736756
train loss:0.04791092288456418
train loss:0.05712503595966795
train loss:0.0392108529323158
train loss:0.028227751566567195
train loss:0.06886550478499126
train loss:0.019384943536963618
train loss:0.02626188279396624
train loss:0.027384416478695512
train loss:0.020796098369610255
train loss:0.03234993341082389
train loss:0.05681527275022988
train loss:0.03350778469978646
train loss:0.04060678376425872
train loss:0.022131812908971753
train loss:0.023862694457883617
train loss:0.07097383894572053
train loss:0.025996473740337027
train loss:0.01691882149899028
train loss:0.05577196806147307
train loss:0.02387289347497521
train loss:0.09312662054580235
train loss:0.024810508374838745
train loss:0.025358762330606938
train loss:0.05086165656072781
train loss:0.008106116533844573
train loss:0.026416769991171887
=== epoch:8, train acc:0.9865894039735099, test acc:0.9708609271523179 ===
train loss:0.02162606632552436
train loss:0.03968895871059339
train loss:0.03407611191356438
train loss:0.03264491581570683
train loss:0.008026776318406948
train loss:0.012614762001607079
train loss:0.012645369445901713
train loss:0.018537429127872984
train loss:0.013599661043655668
train loss:0.04554423637746602
train loss:0.026545754208738264
train loss:0.025757306965520504
train loss:0.03571843891933706
train loss:0.017108136802472536
train loss:0.021476858106731167
train loss:0.02038064177584316
train loss:0.03187057752489742
train loss:0.03123818225263396
train loss:0.025265944326409103
train loss:0.018203195963249354
train loss:0.03743434315836539
train loss:0.02581201292306211
train loss:0.019673369538331657
train loss:0.08003738005243628
train loss:0.017087392456705482
train loss:0.024172845838306558
train loss:0.03637358217853784
train loss:0.018604195068292476
train loss:0.031470892061064376
train loss:0.017756774237152946
train loss:0.022736791193260393
train loss:0.011412685477425624
train loss:0.015014467634348598
train loss:0.021655746694257738
train loss:0.02441815232009847
train loss:0.016389389277599305
train loss:0.016918301647627077
train loss:0.033224670742246915
train loss:0.019328170924328904
train loss:0.02109676853013527
train loss:0.024204265234057224
train loss:0.06580750731761863
train loss:0.018240877983809998
train loss:0.03222714726307505
train loss:0.015156362131153763
train loss:0.009818787099747667
train loss:0.03767080772241005
train loss:0.021455258236556772
train loss:0.015071200428948028
train loss:0.010373282812711479
train loss:0.015699973329761484
train loss:0.009455674053546293
train loss:0.007935419959349817
train loss:0.022866623058512153
train loss:0.008792325451366927
train loss:0.008860723029587448
train loss:0.016836669044237116
train loss:0.013975987254808378
train loss:0.009806719337899849
train loss:0.02228640242223505
=== epoch:9, train acc:0.9892384105960265, test acc:0.9741721854304636 ===
train loss:0.013717080400287834
train loss:0.01615248868710666
train loss:0.012523495081283323
train loss:0.012118108013939127
train loss:0.01709509280261274
train loss:0.015643128704059497
train loss:0.017379491037990993
train loss:0.010265613562924023
train loss:0.007837793584394901
train loss:0.014657790200047104
train loss:0.016317516854864222
train loss:0.03831859732217196
train loss:0.010618189179479815
train loss:0.057591871352868154
train loss:0.021898439270164202
train loss:0.07026541417328698
train loss:0.011105456859764358
train loss:0.026570181452370795
train loss:0.01211863344671612
train loss:0.024994722730293156
train loss:0.010178716784164956
train loss:0.013609439068140937
train loss:0.014730946872048638
train loss:0.009751098101479581
train loss:0.12707053676450608
train loss:0.04316770962212087
train loss:0.009616159463709122
train loss:0.009214717640619095
train loss:0.010777631784058304
train loss:0.02232910506627751
train loss:0.017487249757068658
train loss:0.012702816135557468
train loss:0.021337845547163926
train loss:0.025709730491882177
train loss:0.009814567685362022
train loss:0.00809966142799974
train loss:0.044161926689412496
train loss:0.03180463652323712
train loss:0.03967015551296952
train loss:0.009528116426891975
train loss:0.006528066659565461
train loss:0.03436453926491551
train loss:0.012469690004964604
train loss:0.012732831765477055
train loss:0.053544261403840075
train loss:0.012723288653978912
train loss:0.013378122575293942
train loss:0.03058698405393076
train loss:0.008057156155971454
train loss:0.009826485591975185
train loss:0.006194660430423167
train loss:0.03723998811853284
train loss:0.03056494136443245
train loss:0.00827411968163719
train loss:0.009957479762461
train loss:0.02019089062117558
train loss:0.01000096181237167
train loss:0.023422758105449935
train loss:0.008719220527853478
train loss:0.024150633711577038
=== epoch:10, train acc:0.990728476821192, test acc:0.9735099337748344 ===
=============== Final Test Accuracy ===============
test acc:0.9735099337748344
Saved Network Parameters!
